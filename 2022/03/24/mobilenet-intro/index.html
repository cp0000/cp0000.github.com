<!DOCTYPE html>
<html lang="" class=""><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      计算机视觉移动端小网络：MobileNet
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css">
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>

<article>
    <p class="post-meta">
        <time datetime="2022-03-24 10:48:12 &#43;0000 UTC">
            2022-03-24
        </time>
    </p>

    <h1>计算机视觉移动端小网络：MobileNet</h1>

    <h4 id="mobilenethttpsarxivorgabs170404861"><a href="https://arxiv.org/abs/1704.04861">MobileNet</a></h4>
<p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vison Applications.
MobileNet是为移动端手机和嵌入式设备提出的网络模型。其中MobileNetV1压缩网络的手段主要是利用深度可分离卷积。</p>
<p><strong>深度可分离卷积</strong></p>
<p>标准卷积如下图：</p>
<!-- raw HTML omitted -->
<p>输入一个 12x12x3 的featuremap，经过 5x5x3 的卷积核卷积得到一个 8x8x1 的输出featuremap。如果我们有256个featuremap，则经过卷积操作会得到 8x8x256 个输出featuremap。</p>
<p>深度可分离卷积如下图：</p>
<!-- raw HTML omitted -->
<p>深度可分离卷积与标准卷积的差别是，depthwise-conv将卷积核拆分成单通道形式，在不改变featuremap的深度情况下，对输入的featuremap的每一个通道进行卷积操作，得到和输入featuremap通道数一致的输出特征图。猴通过pointwise-conv，升维。</p>
<p>如上图所示，输入 12x12x3 的featuremap，经过 5x5x1x3 的深度卷积之后，得到了 8x8x3 的输出featuremap。然后在通过 256个 1x1x3 的卷积对 8x8x3 的输入featuremap 进行卷积操作，得到 8x8x256 的输出featuremap。</p>
<p>普通卷积和深度可分离卷积的计算量对比如下图：</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>MobileNetV1 网络详情</strong></p>
<!-- raw HTML omitted -->
<h4 id="mobilenet-v2httpsarxivorgabs180104381"><a href="https://arxiv.org/abs/1801.04381">MobileNet V2</a></h4>
<p><strong>Linear bottleneck</strong></p>
<p>对一个n维空间做ReLU运算，然后对结果进行逆运算恢复，对比ReLU之后的结果与Input相差如下图：</p>
<!-- raw HTML omitted -->
<p>如上图可见，当n=2，3是，与input相比有很大一部分信息丢失了，当n=15，30时，信息多部分被保留。也就是说，对低纬度做ReLU运算，容易造成信息丢失，对高纬度进行ReLU运算，信息丢失会比较少。</p>
<p>以上的发现可以解释MobileNetV1的深度卷积训出来的卷积核有不少是空的这一现象。解决问题的方法是把最后的ReLU6换成Linear。</p>
<!-- raw HTML omitted -->
<p><strong>Inverted residuals</strong></p>
<p>经过DepthWise-Conv的featuremap，输出维度和输入是一致的，使用DW卷积，featuremap无法升维。MobileNetV2中通过在DW卷积之前使用PointWise-Conv来升维，然后在一个更高维的空间中进行DW操作。另外V2中还借鉴了ResNet中的residual block的shortcut结构。</p>
<p>以下是MoblieNet V2的block结构：</p>
<!-- raw HTML omitted -->
<p>这种结构和ResNet的residual block在结构上有相似性，都才用 conv1x1 -&gt; conv3x3 -&gt; conv1x1 的模式，但不同在于：</p>
<ul>
<li>ResNet先降维（0.25 倍），标准卷积，再生维</li>
<li>MobileNet V2先升维（6倍），深度可分离卷积，再降维</li>
</ul>
<p>以下是MobileNetv2 block 和 ResNet residual block的对比：</p>
<p><strong>MobileNet V2 的网络详情</strong></p>
<!-- raw HTML omitted -->
<h4 id="mobilenet-v3httpsarxivorgpdf190502244pdf"><a href="https://arxiv.org/pdf/1905.02244.pdf">MobileNet V3</a></h4>
<p>MobileNet V3的相关技术</p>
<ul>
<li>引入了NAS进行网络结构搜索</li>
<li>引入轻量级注意力模型（SE）</li>
<li>使用了一种新的激活函数 h-swish(x)</li>
</ul>
<p><strong>MobileNet V3 网络结构</strong></p>
<p>网络的结构如下图，引入SE模块：</p>
<!-- raw HTML omitted -->
<p>Pool: average pool 1x1, FC1将通道数减小为1/4，后接一个ReLU函数，FC2将通道数变为原来一样，后接h-sigmoid函数，得到的权重与原来的特质矩阵相乘得到新的特征矩阵。</p>
<p><strong>hard-swish 函数</strong></p>
<!-- raw HTML omitted -->
<p><strong>MobileNet V3 网络详情</strong></p>
<!-- raw HTML omitted -->
<h4 id="总结">总结</h4>
<ul>
<li>MobileNet 在目前的移动端小模型上有着广泛的应用，特别是depthwise-conv，pointwise-conv</li>
</ul>

</article>

            </div>
        </main>
	<script src="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.js"></script>
    </body>
</html>
