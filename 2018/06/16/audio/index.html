<!DOCTYPE html>
<html lang="" class=""><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      音频编解码以及音乐GPU实时可视化效果实现
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css">
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>

<article>
    <p class="post-meta">
        <time datetime="2018-06-16 10:58:22 &#43;0000 UTC">
            2018-06-16
        </time>
    </p>

    <h1>音频编解码以及音乐GPU实时可视化效果实现</h1>

    <h2 id="前言">前言</h2>
<p>在一些音乐播放器上（如上学时候用的播放器<code>千千静听</code>）会有一排 <a href="https://zh.wikipedia.org/zh-hans/%E7%9B%B4%E6%96%B9%E5%9B%BE">直方图</a> 随着音乐节奏忽高忽低的跳动。你会好奇这个直方图的跳动是随着音乐的什么属性来做变换的，貌似有时候和音乐的节奏还挺搭的。要解释清楚这个问题，首先得了解什么声音，以及声音的原理。</p>
<h2 id="声音">声音</h2>
<h3 id="声音的原理">声音的原理</h3>
<p>根据维基百科上所述：声音是一种波动；当演奏乐器、人说话或者敲击桌面时，声音的振动会引起介质——空气分子有节奏的振动，使周围的空气产生疏密变化，形成疏密相间的纵波，这就产生了声波，这种现象会一直延续到振动消失为止。</p>
<p>声音作为波的一种，频率和振幅就成了描述波的重要属性，频率的大小与我们通常所说的音高对应，而振幅影响声音的大小。声音可以被分解为不同频率不同强度的正弦波的叠加。这种变换（或分解）的过程，称为傅立叶变换(Fourier Transform)。</p>
<blockquote>
<p>所以到这里，我们大致明白了音乐播放器上面的直方图其实是声音的频率和振幅的统计图。</p>
</blockquote>
<h3 id="声音的特性">声音的特性</h3>
<ul>
<li><strong>响度（loudness）</strong>：人主观上感觉声音的大小（俗称音量），由“振幅”（amplitude）和人离声源的距离决定，振幅越大响度越大，人和声源的距离越小，响度越大。（单位：分贝dB）</li>
<li><strong>音调（pitch）</strong>：声音的高低（高音、低音），由“频率”（frequency）决定，频率越高音调越高（频率单位Hz（hertz），赫兹)，人耳听觉范围 20～20k Hz。20Hz以下称为次声波，20k Hz以上称为超声波）例如，低音端的声音或更高的声音，如细弦声。频率是每秒经过给定点的声波数量，它的测量单位为赫兹。1000Hz表示每秒经过给定点的声波有1000个周期</li>
<li><strong>音色（Timbre）</strong>：又称音品，波形决定了声音的音色。声音因不同物体材料的特性而具有不同特性，音色本身是一种抽象的东西，但波形是把这个抽象直观的表现。音色不同，波形则不同。典型的音色波形有方波，锯齿波，正弦波，脉冲波等。不同的音色，通过波形，完全可以分辨的。</li>
<li><strong>乐音</strong>：有规则的让人愉悦的声音。噪音：从物理学的角度看，由发声体作无规则振动时发出的声音；</li>
</ul>
<h2 id="音频编解码">音频编解码</h2>
<p>音频的编解码器需要分成硬件，软件两个部分讨论。硬件部分，音频编码器芯片（ADC）会把接收到的声波转换成PCM数据, 由于PCM占用存储资源太大，不利于传输和存储，所以需要利用音频编解码器软件对PCM数据进行压缩，得到如 FLAC/MP3/OGG 的音频文件。当我们播放一段数字音频文件的时候，则需要先进行软解码，也就是对压缩过的音频文件（FLAC/MP3/OGG）先解码成PCM，然后传送解码后的PCM给音频解码器（DAC），解码器将数字信号转换为模拟信号。</p>
<h3 id="音频硬编码过程">音频硬编码过程</h3>
<p>信号的数字化就是将连续的模拟信号转换成离散的数字信号， 一般需要完成采样、量化和编码三个步骤,如下图所示。</p>
<p><img src="http://cp0000.github.io/assets/audio/digital_audio_simpli.png" alt="A/D"></p>
<ul>
<li>采样是指用每隔一定时间间隔的信号样本值序列来代替原来在时间上连续的信号。</li>
<li>量化是用有限个幅度近似表示原来在时间上连续变化的幅度值，把模拟信号的连续幅度变为有限数量，有一定时间间隔的离散值。</li>
<li>编码则是按照一定的规律，把量化后的离散值用二进制数码表示。</li>
</ul>
<p>上诉数字化过程称为脉冲编码调制（Pulse Code Modulation，缩写PCM）</p>
<h3 id="pcm-的码率">PCM 的码率</h3>
<ul>
<li>采样率值 × 采样大小值 × 声道数 bps = 码率</li>
<li>例如: 一个采样率为44.1KHz，采样大小为16bit，双声道的PCM编码的WAV文件，它的数据速率则为 44.1K×16×2 =1411.2 Kbps。这表示存储一秒钟采样率为44.1KHz，采样大小为16bit，双声道的PCM编码的音频信号，需要176.4KB的空间，1分钟则约为10.34M</li>
</ul>
<h3 id="mp3">MP3</h3>
<p><code>MP3</code>全称是<code>MPEG-1</code>or<code>MPEG-2 Audio Layer III</code>，它被设计来大幅降低音频数据量，它舍弃PCM音讯资料中，对人类听觉不重要的资料，从而达到了压缩成较小的档案。而对于大多数用户的听觉感受来说，MP3的音质与最初的不压缩音频相比没有明显的下降。</p>
<h2 id="音频处理">音频处理</h2>
<h3 id="音频傅立叶变换">音频傅立叶变换</h3>
<p>了解了声音和音频编解码的基础知识之后，我们可以开始对音频进行相关应用处理了。那如何处理音频呢？首先用<a href="https://www.audacityteam.org/">Audacity</a>软件打开一段音频，会看到如下图所示的波形图。你会想，这乱糟糟，看起来没啥规律可循的波形的该如何处理。</p>
<p><img src="http://cp0000.github.io/assets/audio/wave.png" alt="wave"></p>
<p>就像我们处理图片的时候，会把像素点颜色分解成<code>RGB</code>三个通道（或者YCbCr,Lab 等其他颜色空间）进行处理一样，在处理音频的时候，我们需要对音频的波形进行分解。根据声音的特性，不同物体发出的声音频率范围是不一样的，如下图是不同乐器的频率范围：</p>
<p><img src="http://cp0000.github.io/assets/audio/imgext.jpeg" alt="frequency"></p>
<p>所以我们可以把音频从时域转换到频域，然后对分解出来的各个频率进行操作。这里需要借助<a href="https://zh.wikipedia.org/zh-hans/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2">傅立叶变换</a>完成音频从时域到频域的转换。</p>
<p>看不懂傅立叶变换的数学公式也没有关系，只要记住下面这张图就好。</p>
<p><img src="http://cp0000.github.io/assets/audio/fft2.png" alt="fft"></p>
<p>如上图，对左边红色的音频时域声波进行傅立叶变换之后，分解成了一系列正弦波。而右边蓝色部分是针对分解的一系列正弦波的统计图，其中横轴是频率，纵轴是振幅。有了这张频谱图，我们可以很方便的处理音频了（如消除某一段频率等等）。</p>
<h2 id="demo">Demo</h2>
<p>有了上面对音频做傅立叶变换后的频率数组，我们可以开始去实现一个音乐实时可视化Demo了。考虑到性能，以及跨平台，我们利用GPU搭配OpenGL编程去实现Demo。实现步骤如下图所示：</p>
<p><img src="http://cp0000.github.io/assets/audio/visualization.png" alt="visualization"></p>
<h3 id="获取pcm数据">获取PCM数据</h3>
<p>在iOS上，利用<code>AVFoundation</code>中的<code>AVAudioEngine</code>获取音乐的<code>pcmbuffer</code>，然后利用<code>Accelerate framework</code>中提供的DSP信号处理api，对<code>pcmbuffer</code>进行傅立叶变换。</p>
<h4 id="加载音乐">加载音乐</h4>
<pre><code>NSString          * audioPath   = [[NSBundle mainBundle] pathForResource: @&quot;bgm&quot; ofType: @&quot;.mp3&quot;];
AVAudioEngine     * engine      = [AVAudioEngine new];
AVAudioFile       * audioFile   = [[AVAudioFile alloc] initForReading: [NSURL fileURLWithPath: audioPath] error: nil];

AVAudioPCMBuffer  * audioPCMBuffer  = [[AVAudioPCMBuffer alloc] initWithPCMFormat: audioFile.processingFormat
                                                                    frameCapacity: (AVAudioFrameCount)audioFile.length];
[audioFile readIntoBuffer: audioPCMBuffer error: nil];

AVAudioPlayerNode * audioPlayerNode = [AVAudioPlayerNode new];
[engine attachNode: audioPlayerNode];

// Connect Nodes
AVAudioMixerNode *mixerNode = [engine mainMixerNode];
[engine connect: audioPlayerNode to: mixerNode format: audioFile.processingFormat];
[audioPlayerNode scheduleBuffer: audioPCMBuffer  completionHandler:^{
    NSLog(@&quot;scheduleBuffer&quot;);
}];

// Start engine
NSError *error;
[self.engine prepare];
[self.engine startAndReturnError: &amp;error];
if (error) {
    NSLog(@&quot;error:%@&quot;, error);
}
</code></pre>
<h4 id="获取-pcmbuffer">获取 PCMBuffer</h4>
<pre><code>int bufferSize = 1024;
[mixerNode installTapOnBus: 0
                bufferSize: bufferSize
                    format: [mixerNode outputFormatForBus: 0]
                   block: ^(AVAudioPCMBuffer * _Nonnull buffer, AVAudioTime * _Nonnull when) {
	float    * magnitudes  = [FFTHelper performFFT: buffer];
}];
</code></pre>
<h4 id="执行-fft">执行 FFT</h4>
<pre><code>+ (float *)  performFFT: (AVAudioPCMBuffer *) buffer {
    if (buffer == NULL) {
        return NULL;
    }
    int frameLength = buffer.frameLength;
    long nOver2 = frameLength / 2;
    vDSP_Length fftLength = nOver2;

    float * imag = (float *) calloc(frameLength, sizeof(float));
    DSPSplitComplex splitComplex;

    splitComplex.realp = buffer.floatChannelData[0];
    splitComplex.imagp = imag;

    int log2n  = (vDSP_Length)(floor(log2((float)frameLength)));
    int radix   = (FFTRadix)kFFTRadix2;
    FFTSetup weights = vDSP_create_fftsetup(log2n, radix);
    vDSP_fft_zip(weights, &amp;splitComplex, 1, log2n, (FFTDirection)FFT_FORWARD);
    float fftNormFactor = 1.0 / (2 * fftLength);
    vDSP_vsmul(splitComplex.realp, 1, &amp;fftNormFactor, splitComplex.realp, 1, fftLength);
    vDSP_vsmul(splitComplex.imagp, 1, &amp;fftNormFactor, splitComplex.imagp, 1, fftLength);
    
    float * magnitudes = (float *) calloc(fftLength, sizeof(float));
    //Convert the fft data to dB
    vDSP_zvmags(&amp;splitComplex, 1, magnitudes, 1, fftLength);
    
    float kAdjust0DB  = 1.5849e-13;
    //In order to avoid taking log10 of zero, an adjusting factor is added in to make the minimum value equal -128dB
    vDSP_vsadd(magnitudes, 1, &amp;kAdjust0DB, magnitudes, 1, fftLength);
    float one = 1;
    // vDSP_vdbcon: Vector convert to decibels, power, or amplitude.
    vDSP_vdbcon(magnitudes, 1, &amp;one, magnitudes, 1, fftLength, 0);

    free(imag);
    vDSP_destroy_fftsetup(weights);
    return magnitudes;
}
</code></pre>
<h4 id="上传fft数据到gpu">上传<code>FFT</code>数据到<code>GPU</code></h4>
<p>拿到频谱数据之后，需要利用<code>shader</code>来实现音乐可视化的效果。当中如何把频谱数据上传到<code>GPU</code>是一个问题，因为频谱数据一个长达1k的数组，直接用<code>uniform float[]</code>来上传，性能很差。这里有一个比较tricky的做法是把频谱数据存储到一张空白图片上，然后把图片转换成<code>texture</code>，再上传<code>texture</code>到<code>GPU</code>，这样就可以优化上传频谱数组性能差的问题。</p>
<pre><code>// Passing the data to the GPU
-（void）load: (float *) data length: (int) length width: (int) width height: (int) height slot: (GLenum) slot {
GLuint    texture   = 0;
GLubyte * raw       = (GLubyte *) malloc (sizeof (GLubyte) * length * 4);

for (int i = 0; i &lt; length; i++) {
    GLubyte color   = GLubyte(data[i] * 255.0);
    raw[4*i+0]      = color; // R
    raw[4*i+1]      = color; // G
    raw[4*i+2]      = color; // B
    raw[4*i+3]      = color; // A
}

glActiveTexture (slot)
glGenTextures (1, &amp;texture)
glBindTexture (GLenum (GL_TEXTURE_2D), texture);

glTexParameteri (GLenum (GL_TEXTURE_2D), GLenum (GL_TEXTURE_WRAP_S), GL_REPEAT);
glTexParameteri (GLenum (GL_TEXTURE_2D), GLenum (GL_TEXTURE_WRAP_T), GL_REPEAT);

glTexParameteri (GLenum (GL_TEXTURE_2D), GLenum (GL_TEXTURE_MAG_FILTER), GL_LINEAR);
glTexParameteri (GLenum (GL_TEXTURE_2D), GLenum (GL_TEXTURE_MIN_FILTER), GL_LINEAR);
glTexImage2D (GLenum (GL_TEXTURE_2D), 0, GL_RGBA, GLsizei (width), GLsizei (height), 0, GLenum (GL_RGBA), GLenum (GL_UNSIGNED_BYTE), raw);
free (raw); raw = NULL;
</code></pre>
<p>}</p>
<h4 id="可视化效果">可视化效果</h4>
<p>最后在<code>shader</code>中解析texture，得到频谱数据，并绘制直方图，就可以做出音乐播放器上的直方图效果啦，如下图：</p>
<p><img src="http://cp0000.github.io/assets/audio/spectrum2.gif" alt="spectrum"></p>
<p>效果比较素，更加华丽的一些关于音乐可视化的效果可以翻翻<a href="http://shadertoy.com/">shadertoy.com</a>，上面有很多绚丽的效果。如下图：</p>
<p><img src="http://cp0000.github.io/assets/audio/spectrum.gif" alt="shadertoy spectrum"></p>
<p>以上。</p>

</article>

            </div>
        </main>
	<script src="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.js"></script>
    </body>
</html>
