<!DOCTYPE html>
<html lang="" class=""><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      Vision Transformer - Transformer的实现和代码解读
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.css">
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-63703524-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-63703524-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-63703524-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>

<article>
    <p class="post-meta">
        <time datetime="2021-10-11 21:05:48 &#43;0000 UTC">
            2021-10-11
        </time>
    </p>

    <h1>Vision Transformer - Transformer的实现和代码解读</h1>

    <h3 id="2-transformer的实现和代码解读">2 Transformer的实现和代码解读</h3>
<h4 id="21-transformer原理分析">2.1 Transformer原理分析：</h4>
<p><img src="http://cp0000.github.io/assets/vit/transformer_1.jpeg" alt="图1: Transformer"></p>
<blockquote>
<p>Encoder</p>
</blockquote>
<p>图1是一个seq2seq的模型，左侧为Encoder block,右侧为Decoder block。红色圈中的部分为Multi-Head Attention,是由多个Self-Attention组成的。Encoder block包含一个Multi-Head Attention, Decoder block包含两个Multi-Head Attention(其中有一个用到Masked)。Multi-Head Attention上方还包括一个Add &amp; Norm 层，Add表示残差连接（Residual Connection）用于防止网络退化。Norm 表示Layer Normalization,用于对每一层的激活值进行归一化。比如说在Encoder input处的输入是机器学习，在Decoder Input处的输入是<!-- raw HTML omitted -->,输出是machine。再下一个时刻在Decoder Input处的输入是machine，输出是learning。不断重复直到输出是句点(.)代表翻译结束。</p>
<p>Encoder: 首先输入 X ∈ R(nx, N)通过一个Input Embedding的转移矩阵 Wx ∈ R(d, nx)变为一个张量，即上文所述的 I ∈ R(d, N)，再加上一个表示位置的Positional Encoding E ∈ R(d, N),得到一个张量。</p>
<p>进入绿色block，绿色的block会重复N次。绿色Block的第1层是一个上文讲的multi-head的attention。现在一个sequence I ∈ R(d, N)经过一个multi-head 的attention，会得到另外一个sequence O ∈ R(d, N).</p>
<p>下一个Layer是Add &amp; Norm，这里的意思是：把multi-head的attention的layer的输入 I ∈ R(d, N)和输出O ∈ R(d, N)进行相加以后，再做Layer Normalization。</p>
<p><img src="http://cp0000.github.io/assets/vit/normalization.jpeg" alt="图2: Normalization"></p>
<p>Batch Normalization强行让一个batch的数据的某个channel的 μ=0，σ=1，而Layer Normalization让一个数据的所有channel的μ=0，σ=1。</p>
<p><img src="http://cp0000.github.io/assets/vit/bn_ln.jpeg" alt="图3: BN和LN的对比"></p>
<p>接着是一个Feed Forward的前馈网络和一个Add &amp; Norm Layer。
所以，这一个绿色的block的前2个Layer操作的表达式为：</p>
<blockquote>
<p>O1 = LayerNormalization(I + Multi-head Self-Attention(I))</p>
</blockquote>
<p>这一个绿色的block的后2个Layer操作的表达式为：</p>
<blockquote>
<p>O2 = LayerNormalization(O1 + FeedForwardNetwork(O1))
Block(I) = O2</p>
</blockquote>
<p>所以Transformer的Encoder的整体操作为：</p>
<blockquote>
<p>Encoder(I) = Block(..Block(Block)(I))</p>
</blockquote>
<blockquote>
<p>Decoder:</p>
</blockquote>
<p>现在来看Decoder的部分，输入包括2部分，下方是前一个timestep的输出的embedding，即上文所述的I ∈ R(d,N)，再加上一个表示位置的Positional Encoding E ∈ R(d,N)，得到一个张量，去往后面的操作。它进入了绿色的block，绿色的block会重复N次。</p>
<p>绿色block里面的内容，首先是Masked Multi-Head Self-attention,masked的意思是使attend on已经产生的sequence,这个很合理，因为还没有产生出来的东西是不存在的，就无法做attention。</p>
<p><strong>输出是：</strong> 对应 i 位置的输出词的概率分布
<strong>输入是：</strong> Encoder 的输出和对应 i-1 位置decoder的输出。所以中间的attention不是self-attention,它的key和value来自Encoder，Query来自上一位置Decoder的输出。</p>
<p><strong>解码：</strong> 这里要特别注意一下，编码可以并行计算，一次性全部Encoding出来，但解码不是一次把所有序列解出来，而是像RNN一样一个一个解出来的，因为要用上一个位置的输入当作attention的query。</p>
<p>明确了解码过程之后最上面的图就很好懂了，这里主要的不同就是新加的attention多了一个mask，因为训练时的output都是Ground Truth，这样可以确保预测第i个位置时不会接触到未来的信息。</p>
<ul>
<li>包含两个Multi-Head Attention层。</li>
<li>第一个Multi-Head Attention层采用了Masked操作。</li>
<li>第二个Multi-Head Attention层的key，value矩阵使用Encoder的编码信息矩阵C进行计算，而Query使用上一个Decoder block的输出计算。</li>
<li>最后有一个Softmax层计算下一个翻译单词的概率</li>
</ul>
<p>下面详细介绍下Masked Multi-Head Self-attention的具体操作，Masked在Scale操作之后，softmax操作之前。</p>
<p><img src="http://cp0000.github.io/assets/vit/masked.png" alt="图4: Masked在Scale操作之后，softmax操作之前"></p>
<p>因为在翻译的过程中是顺序翻译的，即翻译完第i个单词，才可以翻译第 i+1 个单词。通过Masked操作可以防止第i个单词知道第i+1个单词之后的信息。下面以“我有一只猫”翻译成“I have a cat”为例，了解一下Masked操作。在Decoder的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入“<!-- raw HTML omitted -->”预测出第一个单词为“I”，然后根据输入“<!-- raw HTML omitted --> I”预测下一个单词“have”。</p>
<p>Decoder 在训练的过程中使用 Teacher Forcing并且并行化训练，即将正确的单词序列（<!-- raw HTML omitted --> I have a cat）和对应输出(I have a cat <!-- raw HTML omitted -->) 传递到Decoder。那么在预测第i个输出时，就要将第i+1之后的单词掩盖住，注意Mask操作是在Self-Attention的softmax之前使用的，下面用0 1 2 3 4 5分别表示&quot;<!-- raw HTML omitted --> I have a cat <!-- raw HTML omitted -->&quot;。</p>
<p><img src="http://cp0000.github.io/assets/vit/decoder.png" alt="图5: Decoder过程"></p>
<p>注意这里transformer模型训练和测试的方法不同：</p>
<p><strong>测试时：</strong></p>
<ol>
<li>输入<!-- raw HTML omitted -->,解码器输出 I</li>
<li>输入前面已经解码的<!-- raw HTML omitted -->和I，解码器输出have</li>
<li>输入已经解码的<!-- raw HTML omitted -->, I, have, a, cat, 解码器输出解码结束标志位<!-- raw HTML omitted -->,每次解码都会利用前面已经解码输出的所有单词嵌入信息。</li>
</ol>

</article>

            </div>
        </main>
	<script src="https://cdn.jsdelivr.net/npm/han-css@3/dist/han.min.js"></script>
    </body>
</html>
