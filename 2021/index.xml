<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2021s on CP0000 - 一只特立独行的猪</title>
    <link>https://cp0000.github.io/2021/</link>
    <description>Recent content in 2021s on CP0000 - 一只特立独行的猪</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 27 Dec 2021 15:33:23 +0000</lastBuildDate><atom:link href="https://cp0000.github.io/2021/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>我的2021</title>
      <link>https://cp0000.github.io/2021/12/27/my-2021/</link>
      <pubDate>Mon, 27 Dec 2021 15:33:23 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/12/27/my-2021/</guid>
      <description>2021改变了很多又什么都没有 今天是2021年12月27号，再过4天，2021年就结束了，我们将迎来2022年。一年又过去了，回顾2021年，自己好像做了很多事情，比如结婚，买房，升级，但又好像什么都没有做，自身成长并不太多。21年相比20年，我除了老了一岁，自身并没有什么不同。
其实结婚，买房，亦或是职业职级上的晋升，对我而言，可能只是事情发生到某个节点顺理成章的结果，它没有那么令人兴奋，也不会出人意外。假设我今年没有结婚，或者没有买房，我当下的心境估计变化也不大。因为时间是连续的，我不会因为今天我结婚了，生命就发生了大不同；也不会因为新房摇号没摇到，日子就彻底没法过。我们每个人的处境更像是从出生到当下的积分，过往的每一分一秒累加出我们当下的生活状态。所以我不期待某一刻我的生活会发生多么美妙的转折，也多了一分在面对生活中不确定性事件的时候相对泰然的处理。
关于时间 每个人当下的处境是时间积分的结果，它既囊括了个人自身的微观过往，也集合了我们身处宇宙的历史宏观发展。这个时代的弄潮儿有些想冲破地球，移民火星，有些想打破过往教条，建立社会新秩序，他们站在过往人类发展的基石上，运用他们自身的能力和资源去尝试带领大家进步；而我们普通人想冲破时间累积后的状态就很困难，所以为了冲破一成不变乏味的生活，我们有些人会寄希望于宗教，去庙里烧香拜佛，希望菩萨保佑；有些人则选择相信运气，摸奖，嗜赌成性；有些人更加理性，选择所谓的和时间做朋友，日拱一卒，不断的学习成长，精进自我。大家做的这些理智的，非理智的选择，其实都是想突破时间的桎梏，让自己生命泛起一些浪花。而时间确实是一切痛苦的根源。
今年看了一本小书，叫《悉达多》，里面说到时间就像一条河，上游是过去，下游是将来。如果我们站在比时间维度高一维的角度去看，过去，将来，和现在是同时存在的。不论是物理意义上，还是思想意义上，用这个角度去看待时间很高级，有那么一瞬间，它仿佛能让我悟到生命的本质，可以让自己跳脱出当下的处境，以一个更加开阔的思维去看待人生。
那豁达的思维就会让人生变好吗，我想是的，我们绝大多数时候的开心不开心，都是自己的脑袋在作祟。古人的「不以物喜，不以已悲」是非常难的一种状态，也是对「得道」的一种精神状态描述。我们多数人多数时候都会受外界影响自己的情绪，早上为股市大跌而忧愁，下午开会为领导的一个赞扬而雀跃，晚上又因为老婆的抱怨而烦心，每天都在情绪的喜悲中交替，枉费精神。「不以物喜，不以己悲」，让生命趋于平静后的宁静，它值得我去追求。
书影以及其他点滴 今年书大概读了小10本，其中有几本还是很小的那种，读的不多，从书中获得的收获可能也没那么多。有时候我选择看书，坦白讲也和自己对需要获取知识有一种焦虑感有关，强迫自己看的。至于书的选择上，也许自己30多岁了，看的书和自己的世界观也比较接近，书的选择范围比较窄，当然也和读的很少有关。如果要选一本年度书籍的话，推荐《邓小平时代》，这本书让我看到了时代领袖的传奇与坎坷，也让我看到了一个人一生中要面对很多的不确定性，当然最让人津津乐道还是邓小平身上的乐观与坚韧。
而电影方面，就看的更少了，可能是早些年可以看的老电影比较多，加上今年能看的新电影比较少，今年电影看的少，留下很深深刻的电影也很少。这里推荐一部《困在时间里的父亲》，我相信每个看完全剧的人都会被安东尼·霍普金斯最后一段的表现所折服，同时也会思考人之为人的根本；当我们老去时，我们能拥有的或许只剩记忆了，而衰老又会夺走我们的记忆，也许在我老了之后，我不知道自己会不会不知所测。而多数人弥留之际，不知道是不是最后脑海里回想的会和安东尼·霍普金斯一样，是自己的母亲。
今年消费了很多时间在看电子竞技比赛上面，包括了英雄联盟，王者荣耀，Dota2，收获了很多网络流行词，然后就没有其他收获了。我的确很喜欢看竞技类的比赛，比如这类Moba游戏，又比如足球，篮球，网球这类比赛。消费了这么多时间在这类娱乐上，此刻也只能告慰自己人总是要娱乐的，明年要克制一下就好。年度比赛，今年NBA，足球，网球决赛我看的不多，要选的话，本来想选欧洲杯决赛，但我没看，这里选德约科维奇法网半决赛对阵纳达尔那场比赛吧，很精彩。
2021年，自己伤病多了一些，腰自从6月份扭伤，到现在也没痊愈，所以也没怎么好好运动，然后还得了口腔溃疡，咽喉炎这些，中间有一段时间失眠也比较严重。希望2022年，家人和自己身体健康，有了好的体魄，精神才好，生活质量才会高。
最后还是来一句葛优在《甲方乙方》电影里面结尾处的一句台词，2021年过去了，我很怀念它。</description>
    </item>
    
    <item>
      <title>理解VQ-VAE（DALL-E）- Pt.1</title>
      <link>https://cp0000.github.io/2021/12/11/understanding-vqvae/</link>
      <pubDate>Sat, 11 Dec 2021 12:55:42 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/12/11/understanding-vqvae/</guid>
      <description>本文是 https://ml.berkeley.edu/blog/posts/vq-vae/ 一文的翻译。
就像机器学习社群的所有人一样，我们对OpenAI发布的DALL-E的生成结果感到不可思议。这个模型可以根据文本信息生成精确的，高质量的图像。该模型可以渲染出一些在现实世界不存在的问题，比如“牛油果形状的扶手椅”。
为了庆祝DALL-E的发布，我们准备写一个系列文章去解释DALL-E模型的关键部分。
该系列博客的第一篇会来介绍VQ-VAE，VQ-VAE是DALL-E的一部分，它使得该模型可以生成如此多种多样的高质量的图像。
在下一篇博客里面我们将会介绍用于多模态，自回归的模型transformers。transformers是DALL-E的另外一部分，它主要用来学习语言和图像之间的相关性，有了语言和图像之间的相关性，DALL-E才能产生如此有创意且准确的输出。
Note: 这篇博客需要读者了解一些深度学习和贝叶斯概率的知识。
基础 VQ-VAE 是 Vector Quantized Variational Autoencoder 的缩写，这里面有很多的大词，因此，让我们先简要回顾一下基础知识。
我们首先定义隐空间（latent space）的概念，然后再定义自编码器（autoencoder），最后，在解读 VQ-VAE 之前，我们将回顾变分自编码器的基础知识（Note: 如果你对这部分知识很熟悉，请直接跳开这部分，直接去到解读VQ-VAE细节的部分）
隐空间（Latent Space） 隐空间是对给定的原始数据分布的一些潜在的“隐藏”表示。让我们看一些例子来更清楚的理解这个概念。
想象一下原始数据是 x ∈ R_n, 它们是通过线性变换从某个较低维度 z ∈ R_m 生成的（m &amp;lt; n）.具体来说，x 是通过 x =Az + v 来生成的，其中 v 是 n 维独立同分布高斯噪声，A ∈ R_nxm.
一般来说，我们只会看到原始数据x;我们无法访问z，这也是为什么它会被称为数据的潜在或“隐藏”表示。不过，我们想要获得z，因为它是一个更基本的，压缩的数据表示。此外，对于需要使用该数据的算法，这种隐示的数据表示可以成为更为有用的输入。
原始数据是隐空间的一种线性转换的这个设定，和无监督经典算法PCA的定义很像。PCA本质上试图从上面的例子中找到底层的z表示。PCA很棒，但如果潜在表示和原始数据之间有着更复杂的非线性关系呢？举例来说，下图显示了更复杂的隐空间能编码出高层次的信息。在这个例子中，PCA 将无法找到最佳的潜在表示。 相反，我们可以使用自动编码器来找到这个更抽象的潜在空间。
Note: 隐空间不需要是连续的向量空间，它可以是一组离散的变量，这也是VQ-VAE的隐空间类型。但在我们开始之前，让我们先了解一下自编码器。
自编码器（Autoencoders） 自编码器属于无监督学习方法，它使用神经网络为给定的数据寻找非线性的潜在表示。神经网络分成两个部分：一个编码器，z=f(x),一个解码器 x&amp;rsquo; = g(z).
x 是输入数据，z是隐向量，x&amp;rsquo;是 x从隐空间中的重建。f(x) 和 g(z) 同为神经网络。把两个放在一起，整个模型可以被描述为 x&amp;rsquo; = g(f(x))（看下图）
理想情况下，解码器应该能够从编码器的隐表示中准确的重建出原始数据。如果模型能够学习出这种重建，那我们就可以假设我们的隐空间可以很好的代表了原始数据。为了实现这一目标，我们训练一个模型，其中在x和x&amp;rsquo;之间加入重建loss.</description>
    </item>
    
    <item>
      <title>活到33</title>
      <link>https://cp0000.github.io/2021/10/27/33/</link>
      <pubDate>Wed, 27 Oct 2021 19:48:31 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/10/27/33/</guid>
      <description>前几天团建去了海南万宁，回来的最后的一天正好是我33岁生日。每年生日我会写一篇“活到xx”的文章，来“庆祝”生日，今年当然也不例外。在从海口飞回上海的飞机上，我就在想今年要写点什么？
我2011年从大学毕业，今年正好是我毕业10周年的日子，所以我写一下我毕业10年的事情吧。最近大学校友组建了个微信群，拉大家进群让大家回去参加10周年活动；我是没什么意愿回去参加校友会的，个中原因是因为我的大学比较一般，我对我的母校没有很强的归属感。
先说一下工作，毕业10年，我换了三份工作，第一份工作在南宁和广州，干了大概1年。毕业1年之后，12年端午节附近来到上海，在一家小公司开始第二份工作，干了大概2年半，然后14年11月来了腾讯，至今在腾讯已经干了快7年了。三家公司是三种不同的工作体验，其中第二家公司的leader wenqi对我的影响蛮大，跟着wenqi，我学习到了很多研发技术和经验。那两年工作压力很大，记得2012年过年回家，弟弟说我看起来很疲惫，整个人看起来闷闷不乐，也不太爱说话。后来想想那时候我的确给了自己太大的压力，几乎每天都过度焦虑于工作的事情。但那段时间，即便压力再大，也没思考过离职，因为我能感知到每天都在被新的知识冲击，脑袋里面那种获取到新鲜知识的快感让人上瘾。比如尽早return，比如代码结构，第一个new的object，一定是最后一个free，比如markdown，比如状态机，这一个一个经典的编程知识，是一个普通大学毕业生在2012年很难接触到，更别提在日常coding中加以使用。当接触到这些理念之后，我会觉得这东西很有道理，就像是为自己的编程人生找到了锚点。后来在我独自面对一些不确定的问题时，我会变的笃定，有勇气去做一些不确定的事情，相信对的工作方式，相信步步为营，相信“复杂的事情简单化，简单的事情重复做”。
后面即便来了腾讯，我也一直秉持着“复杂的事情简单化，简单的事情重复做”，相信技术是可以自学的。就像后面我自己摸索出iOS reverse，图像图形处理，OpenGL渲染，以及计算机视觉。只要愿意学习，好像的确啥技术都能学。
生活上，这10年，周围的亲人和自己均发生了不小的变化。15年奶奶去世，今年外婆也走了，这两年过年回家发现爸爸妈妈头上的白发和脸上的皱纹也变多了，他们今年53岁，步入了中老年。在万宁的那几天晚上，我不断做梦梦到自己的家人，因为一些外在原因，我高中之后，就没有和家人长时间生活在一起，每年基本上就过年那几天会一起生活；有时候我会觉得他们是一瞬间老的，印象中他们还是意气风发的青年人，有着自己的抱负，转瞬就已经步入中老年了。我和我的父辈，这两代农村人，因为打工，求学错过了很多和亲人一起生活的时光。今年国庆期间，我在老家办了婚礼，爸爸妈妈很开心，我的人生也进入了新的一个阶段。
活到33，有一点30而立的意思，希望在今后的日子里，自己可以更加笃定自信，生命有很多的可能性，勇敢去追求自己的内心。</description>
    </item>
    
    <item>
      <title>Vision Transformer - Transformer的实现和代码解读</title>
      <link>https://cp0000.github.io/2021/10/11/vit-2/</link>
      <pubDate>Mon, 11 Oct 2021 21:05:48 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/10/11/vit-2/</guid>
      <description>2 Transformer的实现和代码解读 2.1 Transformer原理分析： Encoder
图1是一个seq2seq的模型，左侧为Encoder block,右侧为Decoder block。红色圈中的部分为Multi-Head Attention,是由多个Self-Attention组成的。Encoder block包含一个Multi-Head Attention, Decoder block包含两个Multi-Head Attention(其中有一个用到Masked)。Multi-Head Attention上方还包括一个Add &amp;amp; Norm 层，Add表示残差连接（Residual Connection）用于防止网络退化。Norm 表示Layer Normalization,用于对每一层的激活值进行归一化。比如说在Encoder input处的输入是机器学习，在Decoder Input处的输入是,输出是machine。再下一个时刻在Decoder Input处的输入是machine，输出是learning。不断重复直到输出是句点(.)代表翻译结束。
Encoder: 首先输入 X ∈ R(nx, N)通过一个Input Embedding的转移矩阵 Wx ∈ R(d, nx)变为一个张量，即上文所述的 I ∈ R(d, N)，再加上一个表示位置的Positional Encoding E ∈ R(d, N),得到一个张量。
进入绿色block，绿色的block会重复N次。绿色Block的第1层是一个上文讲的multi-head的attention。现在一个sequence I ∈ R(d, N)经过一个multi-head 的attention，会得到另外一个sequence O ∈ R(d, N).
下一个Layer是Add &amp;amp; Norm，这里的意思是：把multi-head的attention的layer的输入 I ∈ R(d, N)和输出O ∈ R(d, N)进行相加以后，再做Layer Normalization。
Batch Normalization强行让一个batch的数据的某个channel的 μ=0，σ=1，而Layer Normalization让一个数据的所有channel的μ=0，σ=1。
接着是一个Feed Forward的前馈网络和一个Add &amp;amp; Norm Layer。 所以，这一个绿色的block的前2个Layer操作的表达式为：</description>
    </item>
    
    <item>
      <title>Vision Transformer - Self-Attention</title>
      <link>https://cp0000.github.io/2021/10/10/vit/</link>
      <pubDate>Sun, 10 Oct 2021 10:04:18 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/10/10/vit/</guid>
      <description>Transformer是Google团队在2017年提出的一种NLP经典模型，目前流行的Bert也是基于Transformer。Transformer模型使用Self-Attention机制，不采用RNN的顺序结构，使得模型可以并行化训练，而且能够拥有全局信息。
1. Self-Attention 1.1 处理Sequence数据模型 Transformer是一个Sequence to Sequence model,特别之处在于它使用self-attention.要处理一个Sequence，最常想到的是使用RNN，它的输入是一串vector sequence,输出是另一串vector sequence.如下图1左所示：
假设是一个single directional的RNN，当输出b4s时，默认a1,a2,a3,a4都已经看过了。如果假设是一个bi-directional的RNN，那当输出 b任意 时，默认a1,a2,a3,a4都已经看过了。RNN非常擅长处理input是一个sequence的状况。
那RNN有什么样的问题呢？RNN的问题在于，它很不容易并行化。假设在single directional的RNN情况下。要算出b4就必须要先看a1,再看a2,再看a3,再看a4,所以这个过程很难并行处理。
所以就有人拿CNN来取代RNN， 如下图1右所示。其中，橘色的三角形表示一个filter，每次扫过3个向量a,扫过一轮以后，就输出一排结果，使用橘色的小圆点表示。
这是第一个橘色filter的过程，还有其他的filter，比如图2中的黄色的filter，它经历着与橘色的filter相似的过程，又输出一排结果，使用黄色的小圆点表示。
所以使用CNN的确可以做到跟RNN的输入输出相似的关系，也可以做到输入是一个sequence，输出是另外一个sequence。 但是表面上CNN和RNN可以做到相似的输入和输出，但是CNN只能考虑非常有限的内容。比如在我们右侧的图中CNN的filter只考虑了3个vector，不像RNN可以考虑之前所有的vector。CNN可以通过堆叠filter，多堆叠几层，上层的filter就可以考虑比较多的信息，比如第二层的蓝色三角形filter看了6个vector，所以只要叠很多层，就能够看很长时间的信息。
而CNN的一个好处是：它是可以并行化的，不需要等橘色的filter算完，再算黄色的filter。但是必须要叠很多层filter，才可以看到长时的资讯。所以self-attention，如下图3所示，目的是使用self-attention layer取代RNN所做的事情。
所以重点是：我们有了一种新的layer，叫self-attention,它的输入输出和RNN是一样的，输入一个sequence，输出一个sequence，它的每一个输出b1 - b4 都看过了整个输入sequence，这一点与bi-directional RNN相同。但神奇的地方是：它的每一个输出b1-b4可以并行计算。
1.2 Self-attention Self-attention 具体是怎么做的？
首先假设我们的input是图4的x1-x4,是一个sequence，每一个input（vector）先乘上一个矩阵W得到embedding，即向量a1-a4。接着这个embedding进入self-attention层，即每一个向量a1-a4分别乘上3个不同的transformer matrix Wq, Wk, Wv,以向量a1为例，分别得到3个不同的向量q1,k1,v1.
接下来使用每个query q 去对每个key k做attention，attention就是匹配这2个向量有多接近，比如我现在要q1 和 k1 做attention，就可以把2个向量做scaled inner product，得到α1,1.接下来再拿q1和k2做attention，得到α1,2,以此α1,3,α1,4。其中scaled inner product计算公式为：
d 是q跟k的维度。其中q.k的数值会随着dimension的增大而增大，所以要除以dimension的根号值，相当于归一化结果。
接下来要做的事如图6所示，把计算得到的所有α1,i值取softmax操作。
取完softmax操作以后，我们得到了α^1,i，我们用它和所有的vi值进行相乘。具体来讲，把α^1,1 乘上v1, α^1,2 乘上v2,把α^1,3乘上v3,把α^1,4 乘上v4,把结果加起来得到b1，所以，在产生b1的过程中用了整个sequence的信息。如果要考虑local的information，则只需要学习出相应的α^1,i = 0,b1 就不再带有那个对应分支的信息了；如果要考虑global的information，则只需要学习出相应的^1,i ≠ 0，b1就带有全部的对应分支信息了。
同样的方法，也可以计算出b2,b3,b4,如下图8所示，b2就是拿query q2去对其他的k做attention，得到α^2,i，再与value值vi相乘取weighted sum得到的。
经过了以上一连串计算，self-attention layer做的事情跟RNN是一样的，只是它可以并行的得到layer输出的结果，如图9所示。现在我们要用矩阵表示上述的计算过程。
首先输入的embedding是 I = [a1, a2, a3, a4], 然后用 I 乘以 tranformation matrix Wq 得到 Q = [q1, q2, q3, q4],它的每一列代表着一个vector q。同理，用I乘以tranformation matrix Wk 得到 K = [k1, k2, k3, k4], 它的每一列代表着一个vector k。用 I 乘以 tranformation matrix Wk 得到 Q=[v1, v2, v3, v4],它的每一列代表着一个vector v。</description>
    </item>
    
    <item>
      <title>微习惯</title>
      <link>https://cp0000.github.io/2021/07/25/mini-habits/</link>
      <pubDate>Sun, 25 Jul 2021 21:03:54 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/07/25/mini-habits/</guid>
      <description>微习惯 - 书摘 压力越多，生活就越难改变。 一个行为变成习惯所需的时间平均为66天 2009年的那项研究里还有一点更有趣的记录。研究人员得出了一个结论：从生理学角度看，漏掉一天也不会耽误习惯的养成，一天成就不了这个过程，也毁灭不了这个过程；但从心理学角度看，这可能会带来问题。如果你真的漏掉了一天，请记住，以后要尽量坚持下去，一天都不要偷懒，因为坚持才能防止你受到打击并前功尽弃。 重复就是（潜意识）大脑使用的语言。多重复几遍，就不会忘记。这是最初也是最后一个关于大脑的笑话。 大脑是由执行决策和进行自动行为模式识别的两部分组成的系统。前额皮层的管理功能相当活跃，反应灵敏，但同时也消耗了大量的精力（和意志力）。基底神经节的自动功能不仅强大，而且效率高。它们能节省精力，无须持续监督就能处理各种任务。 边际效用递减法则。这条经济法则认为，吃第五块比萨时的愉悦感会略低于吃第四块时的，吃第四块时的又略低于吃第三块时的。我们在进行重复行为时也存在同样的现象。 我们是用动力或意志力开启新的行为。动力不可靠，所以不能充当建立习惯的策略。意志力可靠，但前提是你没有把它耗尽。 引起意志力损耗的5大主要因素：努力程度，感知难度，消极情绪，主观疲劳和血糖水平。如果我们能成功客服这5项障碍，我们就应该能走向成功。 塑造你生活的不是你偶尔做的一两件事，而是你一贯坚持做的事。 一小步+想做的事=较高的进一步行动的可能性。 养成每天做一个俯卧撑的习惯比隔很久才狂做30个俯卧撑更好。只有先养成习惯，才能将其培养得更强，目标更高。 一个普遍真理的具体案例-人一旦感受到控制，就会消极怠工。 当人们认为任务和决定很有趣时，他们的毅力就会更强。 自主权似乎是通过激活我们的内在动力起作用的，其中涉及的案例显示，自主权越高，结果越好（每一项都有独立的研究支持），这些案例包括肥胖症患者减肥、吸烟者戒烟以及糖尿病患者控制血糖水平。 幸福感会促使人们进行抽象思考，这有助于看清全局，但会不利于实现目标，因为实现目标需要具体思维。 正念是目标清晰地活着和敷衍地活着之间的区别。 一个得不到执行的念头只会消亡。 我的经验法则是把我想要的习惯缩小，直到小得不可思议为止。当某件事听起来“小得不可思议”时，大脑会认为它毫无威胁。 用“为什么钻头”找到来源。列好习惯后，看看你为什么想要实现它们，但别在这一步就停止。再问问为什么，不断地问下去，直到形成循环和重复为止，因为这时候你已经找到了核心。想让这个方法奏效，就必须诚实地回答问题，所以请深入挖掘。 我们的目标是进入执行心态，而不是被困在慎重心态的层面。 大笑，大笑时会释放出让你心情变好的化学物质。我有时在完成写作任务后会在YouTube上看读错唇语的滑稽视频。下次有人看到你在看一段搞笑视频时，告诉他们你在训练大脑吧，这可是科学！ 成功会带来更多成功，因为我们喜欢这种结果和成就感。 过程中让自己感到快乐。快坚持不下去时，就给自己一点奖励，休息一下。 一项研究显示，把想法（无论积极还是消极）写在纸上时，会让其在大脑中更加突出，而打字就不具备同样的效应。一定要手写，才能将其重要性放大。 经常对自己有较高的期待值是很有价值的，因为这能提高你的目标上限。 这个策略的全部益处，力量和优势都取决于你在纸面上和心里始终将目标保持在微小状态的能力 代表行为已成为习惯的信号有： 没有抵触情绪：该行为似乎做起来容易，不做反而更难。 身份：现在你认同该行为，而且可以信心十足地说“我常看书”或“我是个作家”。 行动时无须考虑：你不需要做出执行的决定就能开始该行为。你不会想：“好吧，我决定去健身。”你会自然地收拾好东西并出发，这是因为到周二了，或者好像到该运动的时间了。 你不再担心了：刚开始时，你也许会担心自己漏掉一天或者早早放弃，可当行为变成习惯后，你知道你会一直做这件事，除非出现紧急情况。 常态化：习惯是非情绪化的。一旦一件事成为习惯，你不会因为“你真的在做这件事”而激动不已。当一个行为变为一种常态，它就是习惯了。 它很无聊：好的习惯并不会让人兴奋，它们只是对你有好处而已。你会因为它们而对生活更有激情，但别指望行为本身也是如此。 从小事开始和消除期待值带来的压力是我们取得成功的秘诀，而且效果很好，所以我们要尽可能长久保持。 李小龙有一句名言能很好地总结这一点：“要满意，但别满足。”李小龙在他32年有限生命里的成就比两个普通人在80年里的成就之和还要多，所以听他的没错。 只要遇到抵触，我就会把任务缩小，问题就解决了 戒烟很容易，戒烟很容易，戒烟很容易。 只要往小处想，你就会成为自己生活的主宰。 如果你急于取得巨大进步，那就把精力投入超额完成任务中。大目标在纸面上看着漂亮，但只有行动才算数。目标渺小、结果丰硕的状态比反过来好多了。 你越精通微习惯策略，在生活方方面面收获的成功就会越多 </description>
    </item>
    
    <item>
      <title>法官大人,无罪之最,为全人类S2 简评</title>
      <link>https://cp0000.github.io/2021/05/16/meiju2021-h1/</link>
      <pubDate>Sun, 16 May 2021 11:39:33 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/05/16/meiju2021-h1/</guid>
      <description>最近看的几部美剧，简单记录。
法官大人 - 🌟🌟🌟 Mr.White换了身西装变成了新奥尔良地区的大法官。这部剧我给个三星，刚及格的水平。剧情比较缓慢，不合理的地方偏多，人物冲突让人high的场景几乎没有。剧中角色也比较单薄，你能才到每个人的行为方式，没有惊喜。唯一的亮点是该剧发生在新奥尔良，我们以往很少在美剧电影中看到新奥尔良，不过可惜的是，剧中对新奥尔良的刻画也不够好，平淡。
无罪之最 - 🌟🌟🌟🌟 这是一部netflix出品的西班牙悬疑电视剧，故事发生在西班牙巴塞罗那和马拉加。男主角在大学的一次派对上，失手杀死了另外一个年轻人，不幸做了四年牢。故事一开始悬疑拉的满足的，男主角的老婆女主角，说因为工作要去柏林，男主角送走女主角之后，却收到了女主角手机发来的别人拍摄的女主角较为裸露的各种视频，尝试电话也打不通。个中故事不多说，这里说下女主角，女主角的身世比男主角悲惨的多，墨西哥穷苦乡村出身，没有父亲，母亲吸毒，死了之后，女主被送到孤儿院。青少年时期，女主从孤儿院逃了出来，从墨西哥逃跑到了西班牙，却没有联系上在西班牙的表哥。流浪了两年之后，在被警察准备遣返的路上，被一个开脱衣舞厅的叫汉尼拔拦了下来，从此就开始一段悲惨的脱衣舞女和妓女的生活。 脱衣舞女生涯中，解释了另外三个舞女，其中一个在一次聚众性爱中，不幸被本剧的反派失心疯杀死。那件事情之后，剩下的三个舞女决心逃离这个地方。一通计划之后，其中两个人干掉了汉尼拔，并制造了女主死亡的假象，逃了出来。10年之后，女主已经换掉身份，被和男主幸福的生活在一起，但无奈男主失手杀人留下了祸根，由此往事被掀起。 我是从一个播客中得知这部剧，播客的名字大致是无罪之最是不是悬疑片的最终形态。影片由他的独特之处，每集开篇都是剧中人物的POV，讲述自己的生活和心路历程。但实话实说，剧进行到最后，戏剧性不太够，悬疑被解答之后，也没有特别的快感，虽然有一些反转，但不够过瘾，对于结局，即使没有完全猜到，但也不会出乎意料，合乎情理。
整体给个四星吧。
为全人类 第二季 - 🌟🌟🌟🌟 故事承接第一部的发展，讲述了美国人登月故事。不过这部剧有一个巨大的假象，苏联人是第一个登上月球的，且苏联也没有解体。总体来讲，这部剧的优点在于摄影，无论是太空场景，还是宇航员在月球上的作业，抑或是地球上的主角们开的车，都非常的fancy。这部剧是apple tv出品，政治非常正确，剧中重要角色几乎都是女性主导，有色人种参与的也非常多。男主角在第二部戏份其实不多，老婆还出轨，的确有点悲催，男二也是离婚，老婆找了个富翁，第二部的结尾男二也挺惨。 想多说一点的是，Apple TV出品的几部剧，总能能剧中感受到苹果所传达出来的价值观，就是那种很正，中产，smart，注重视觉体验。评价四星，闲暇时间看看养养眼也不错。</description>
    </item>
    
    <item>
      <title>投资最重要的事</title>
      <link>https://cp0000.github.io/2021/05/16/touzizuizhongyaodeshi/</link>
      <pubDate>Sun, 16 May 2021 11:13:03 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/05/16/touzizuizhongyaodeshi/</guid>
      <description>从书名也能知道这是一本关于投资的书籍。会去阅读这本书的契机是在一档叫BYM的博客里面主播分享了这本书。 这本书我读的很慢，断断续续持续了得有个半年了。有趣的是在读这本书的过程中，美股股市由牛市转换为最近的连续大跌。我当然也没能躲过这次大跌，今年的盈利全部回吐，并亏损。
所以投资最重要的事情到底是什么？这里我罗列一些我阅读过程中标记的原书段落
投资是一门科学，更是一门艺术，而投资艺术和其他艺术一样，要想卓越出众，既要多用脑，又要多用心；如何用脑来进行与众不同的第二层次的思考和决策，想法比一般人更深一层；二是如何用心来坚持与众不同的第二层次的思考和行动，行动比一般人做得更高一层。 除了问自己如何以及为何应该成功，在思考一下，别人为什么会失败。 即使是最顶尖的投资者，也会常常犯错。 周期的极端性主要源自人类的情绪与弱点，主观与矛盾。 极端市场行为会发生逆转。相信钟摆朝着一个方向永远摆动或永远停留在断点的人，最终将损失惨重。了解钟摆行为的人则将收益无穷。 逆向投资 买进人人喜欢的股票不会赚大钱，买进被大众低谷的股票才会赚大钱 看到别人没有看到或不重视的品质 多数时候人们会根据既往来预测未来，预测的价值很小。 人类不是被一无所知的事所累，而是被深信不疑的事所累。 对自身处境的正确认识会为我们了解未来事件，采取相应对策提供宝贵的洞见。 忘记过去的人注定会重蹈覆辙 谨慎能帮助我们避免错误，但它也会阻止我们取得伟大的成就。 其他投资者无忧无虑时，我们应小心翼翼；当投资者恐慌时，我们应变得更加积极。 这本书这次我读的不深，要说这次阅读带给我的新知识，新经验。应该是更加强调了投资是一件很严肃，需要认真对待的事情。当我们进入到投资市场时，对我们的投资行为，是需要花费精力去思考自己为什么会投资这一支股票，我的底层逻辑是什么？
很多和我一样的普通人，很难去看到一些当下不明朗，不清晰的机会，即使我们蒙对了一些股票，但在不清楚的状态下我们会谨慎投资，一般仓位都比较小。所以在不清楚的状态下想靠一次投资达成财务自由非常困难。
所以投资最重要的事情是去认真思考自己每一笔投资，想清楚自己当下投资的原因。前期肯定会遭遇一些失败，但经过不断复盘，优化。我们会慢慢笃定我们的一些操作。而这个时候可能也就离成功的投资不远了。</description>
    </item>
    
    <item>
      <title>Have you no sense of decency?</title>
      <link>https://cp0000.github.io/2021/05/04/no-decency/</link>
      <pubDate>Tue, 04 May 2021 15:13:26 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/05/04/no-decency/</guid>
      <description>最近在某个播客里面听到一段关于美国麦卡锡主义是如何被打败的故事。其中1954年，当麦卡锡把魔抓伸向军队的时候，美军律师约瑟夫韦尔奇受够了来自麦卡锡参议员各种的没有实际证据的凭空指控，听证会上说了那句著名的“Have you no sense of decency, sir”，就此，麦卡锡主义的影响开始消融。这当然有演绎这句话的成分，但从中也能体会到decency是当时一些人很在意的东西，它能够引起社会上大多数人的共鸣。
那什么是Decency，问这个问题，是因为 decency 对我来讲是一个很陌生的词。Decency如何影响人的行为在我们的现实生活也几乎难寻踪迹。 Decency,中文翻译为体面，什么是体面，我们平日生活中的爱面子，讲排场是一种体面嘛？我觉得不是，爱面子，讲排场有部分原因是虚荣，而体面至少是个中性偏褒的词汇。Decency 在维基百科页面是转链接到 respect 页面，所以在英文中 decency，体面，表达的是一种尊重和尊敬。
那我们目前处在的这个社会，大家讲究decency吗？我想应该是蛮多人不讲究是否体面的，而且这种不关心，不在意日益变得严重。讲几个例子， 我07年上大学的时候，玩魔兽争霸3，在游戏平台和别人对战，胜负已定的情况下，失利的一方一般会礼貌性的打出gg,也就是good game，无论是虚假的，还是真心的，都要说一句对方打的不错，投子认负。而现在在玩游戏的时候，如王者荣耀，吃鸡类的游戏中，大家队友之间都要开语音互喷，更加别说给予对手尊重了。
在现实生活工作中，我也很难感受到 decency。一些底层工人我们可以说是经济基础决定了他们无法做到体面，我所在的互联网职场是个高薪行业，但也不存在什么decency一说。近些年国内各大厂基本上都在执行996，甚至有一个资本开设的播客就叫 captita 996l，来宣扬这种加班奋斗的行为。还有今年早些时候PDD发生的员工被辞退的事件，一个年入至少30w的人，过程中，爆出这个同学手机电脑等各种隐私被查个底朝天，这个人就像被当作犯罪分子一般对待。你说，这里面有丝毫的decency吗？我们不被公司体面的对待也就算了，彼此之间也互相轻薄，自古文人相轻，技术人员也互相轻视，当讨论技术的时候，在听懂了你的技术之后，很容易就来一句你这不就是xxx，很简单的之类的。
那其他行业就有体面可言么？我经验很有限，了解到的如教育行业，建筑行业，医疗行业，都难谈体面。既然整个群体都不关心是否体面，那是因为经济原因吗？我们处在初级阶段，还没到讲体面的时候？当然，体面和物质肯定有一定的联系，它肯定是在物质得到一定自由之后，人类在追求内心幸福的时候研究出来的一种形而上的东西。但我觉得体面和物质（资本）之间并不是一个线性相关的联系。我在祖辈身上看到的一些体面行为，在我们这辈身上，却难寻踪迹。
所以写到这里，我也没搞清楚，为啥我们变得看起来越来越不讲究体面。是因为现在生活的快节奏，我们没有精力去维持一种体面？还会整个社会日益增长的财富不均，让大多数人散失了维持体面的动力？我无从知道答案，只希望我们每个人都能够被体面的对待，have a little sense of decency。</description>
    </item>
    
    <item>
      <title>我的2020</title>
      <link>https://cp0000.github.io/2021/01/10/my2020/</link>
      <pubDate>Sun, 10 Jan 2021 20:38:38 +0000</pubDate>
      
      <guid>https://cp0000.github.io/2021/01/10/my2020/</guid>
      <description>关于新冠疫情 2020年，英文口语一般读twenty-twenty，可以看作是一个叠音词，那2020年，她可爱吗？很显然，twenty-twenty一点都不可爱，2020年可能是我们目前这些生活在和平年代和地区的人过的最荒诞的一年。而且这种由疫情带来的荒诞，萧条，后遗症至今仍在延续。2020年年初新冠病毒在武汉爆发，接着疫情席卷全球，时至今日，诸如美国，英国还在爆发大规模的疫情，经过一年的抗疫，这个星球至今还没有从疫情中走出来。疫情带来的影响力是持续的，虽然我们国家疫情在7，8月份基本得到了控制，但时至今日，大家生活的方方面面还是笼罩在后疫情时代。我们无法自由的出行，随时随地都要带着口罩，全球化彻底的完蛋。可以说疫情撕掉了这个世界一些发达国家的伪装，我们真实的了解了这个世界的人们原来的样子。不免感概，原来所谓的发达国家，人性都是一样的，有钱会让人变的善良，但如果自身利益受损，有钱人会果断撕掉伪装，拼命的保护着自己的利益。 但真相并不是每个人都想要，希望疫情能够早点过去，让世界恢复他原来的样子，哪怕是伪装的样子也好，撕破脸皮，赤身肉搏，真的是让人太绝望了。
电影，书，体育 2020年，我看了一些电影，美剧，书籍看的不多。书籍推荐《史蒂夫·乔布斯传》，这本书早在2012年，我就打算去看，但那个时候没有看进去，20年读这本书，收获蛮多的，更加加深了我多乔布斯的尊敬。电影方面，今年没看到特别好的电影，都比较一般把。美剧方面收获多一些，比如《风骚律师》，《黄石》都不错。
体育比赛方面，2020年也没看到很精彩的比赛，主要原因还是新冠病毒，很多足球，篮球，网球的比赛，没有观众，观看起来差点意思。
生活 20年5月份，自己去医院做了一个ACL手术，至今也已经快8个月，我的腿还在恢复中。目前还没有恢复到韧带撕裂之前的状态，单腿还不太敢发力。不过相比手术之前，还是有很多进步的，我可以跑步，踩动感单车了，暴汗的感觉还是很不错的。
一点感想 生老病死，岁月蹉跎，2021疫情还在延续，这个世界或许不可能回到之前的样子，或许我们还要在疫情的笼罩下继续生活到2022。只是希望自己能够不以物喜，不以己悲，岁数长了，智慧也要涨。无论晴天雨天，冬天夏天，希望自己可以笃定思想，稳步前行。</description>
    </item>
    
  </channel>
</rss>
